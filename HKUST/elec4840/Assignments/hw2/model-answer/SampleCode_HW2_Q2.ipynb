{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    ")\n",
    "from monai.metrics import DiceMetric,SurfaceDistanceMetric,HausdorffDistanceMetric,MeanIoU\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train/validation/test data filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Your/Path/To/Datafolder\"\n",
    "train_images = sorted(glob.glob(os.path.join(data_dir,\"train\" ,\"image\", \"*.nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_dir,\"train\" , \"mask\", \"*.nii.gz\")))\n",
    "train_data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "\n",
    "val_images = sorted(glob.glob(os.path.join(data_dir,\"val\" ,\"image\", \"*.nii.gz\")))\n",
    "val_labels = sorted(glob.glob(os.path.join(data_dir,\"val\" , \"mask\", \"*.nii.gz\")))\n",
    "val_data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(val_images, val_labels)]\n",
    "\n",
    "test_images = sorted(glob.glob(os.path.join(data_dir,\"test\" ,\"image\", \"*.nii.gz\")))\n",
    "test_labels = sorted(glob.glob(os.path.join(data_dir,\"test\" , \"mask\", \"*.nii.gz\")))\n",
    "test_data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(test_images, test_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data augmentation\n",
    "\n",
    "For data augmentation, here are the basic requirements:\n",
    "\n",
    "1. `LoadImaged` loads the spleen CT images and labels from NIfTI format files.\n",
    "1. `EnsureChannelFirstd` ensures the original data to construct \"channel first\" shape.\n",
    "1. `ScaleIntensityRanged` clips the CT's data format, HU value, into a certain range (-57,164) and normalize it to (0,1)\n",
    "1. `CropForegroundd` removes all zero borders to focus on the valid body area of the images and labels.\n",
    "1. `RandCropByPosNegLabeld` randomly crop patch samples from big image based on pos / neg ratio.  \n",
    "The image centers of negative samples must be in valid body area.\n",
    "\n",
    "You can try more data augmentation techniques to further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class CT_Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=None,split='test'):\n",
    "        self.data=dataset_path\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_dict = self.data[idx]\n",
    "        if self.transform:\n",
    "            data_dict = self.transform(data_dict)\n",
    "        return data_dict\n",
    "        \n",
    "\n",
    "\n",
    "# here we don't cache any data in case out of memory issue\n",
    "train_ds = CT_Dataset(train_data_dicts,train_transforms,split='train')\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=16)\n",
    "val_ds = CT_Dataset(val_data_dicts,val_transforms,split='val')\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "test_ds = CT_Dataset(test_data_dicts,val_transforms,split='test')\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a 3D UNet for segmentation task\n",
    "\n",
    "We give a possible network structure here, and you can modify it for a stronger performance.\n",
    "\n",
    "In the block ```double_conv```, you can implement the following structureï¼š\n",
    "\n",
    "| Layer |\n",
    "|-------|\n",
    "| Conv3d |\n",
    "| BatchNorm3d |\n",
    "| PReLU |\n",
    "| Conv3d |\n",
    "| BatchNorm3d |\n",
    "| PReLU |\n",
    "\n",
    "\n",
    "In the overall UNet structure, you can implement the following structure. ```conv_down``` and ```conv_up``` refers to the function block you defined above.\n",
    "\n",
    "| Layer | Input Channel | Output Channel |\n",
    "|-------|-------------|--------------|\n",
    "| conv_down1 | 1 | 16 |\n",
    "| maxpool | 16 | 16 |\n",
    "| conv_down2 | 16 | 32 |\n",
    "| maxpool | 32 | 32 |\n",
    "| conv_down3 | 32 | 64 |\n",
    "| maxpool | 64 | 64 |\n",
    "| conv_down4 | 64 | 128 |\n",
    "| maxpool | 128 | 128 |\n",
    "| conv_down5 | 128 | 256 |\n",
    "| upsample | 256 | 256 |\n",
    "| conv_up4 | 128+256 | 128 |\n",
    "| upsample | 128 | 128 |\n",
    "| conv_up3 | 64+128 | 64 |\n",
    "| upsample | 64 | 32 |\n",
    "| conv_up4 | 32+64 | 32 |\n",
    "| upsample | 32 | 32 |\n",
    "| conv_up4 | 16+32 | 16 |\n",
    "| conv_out | 16 | 2 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "    nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "    nn.BatchNorm3d(out_channels),\n",
    "    nn.PReLU(),\n",
    "    nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "    nn.BatchNorm3d(out_channels),\n",
    "    nn.PReLU())\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.conv_down1 = double_conv(1, 16)\n",
    "        self.conv_down2 = double_conv(16, 32)\n",
    "        self.conv_down3 = double_conv(32, 64)\n",
    "        self.conv_down4 = double_conv(64, 128)       \n",
    "        self.conv_down5 = double_conv(128, 256)\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)        \n",
    "        \n",
    "        self.conv_up4 = double_conv(128 + 256, 128)\n",
    "        self.conv_up3 = double_conv(64 + 128, 64)\n",
    "        self.conv_up2 = double_conv(64 + 32, 32)\n",
    "        self.conv_up1 = double_conv(32 + 16, 16)\n",
    "        \n",
    "        self.last_conv = nn.Conv3d(16, n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv_down1(x)  \n",
    "        x = self.maxpool(conv1)    \n",
    "        \n",
    "        conv2 = self.conv_down2(x) \n",
    "        x = self.maxpool(conv2)    \n",
    "        \n",
    "        conv3 = self.conv_down3(x)  \n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        conv4 = self.conv_down4(x)  \n",
    "        x = self.maxpool(conv4)      \n",
    "        \n",
    "        x = self.conv_down5(x)\n",
    "        x = self.upsample(x)  \n",
    "        \n",
    "        x = torch.cat([x, conv4], dim=1) \n",
    "\n",
    "        x = self.conv_up4(x) \n",
    "        x = self.upsample(x) \n",
    "        \n",
    "        x = torch.cat([x, conv3], dim=1) \n",
    "        \n",
    "        x = self.conv_up3(x) \n",
    "        x = self.upsample(x) \n",
    "        \n",
    "        x = torch.cat([x, conv2], dim=1) \n",
    "\n",
    "        x = self.conv_up2(x)\n",
    "        x = self.upsample(x)   \n",
    "        \n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        \n",
    "        x = self.conv_up1(x)\n",
    "        \n",
    "        out = self.last_conv(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:1\")\n",
    "model = UNet(n_classes=2).to(device)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "asd_metric = SurfaceDistanceMetric(include_background=False, distance_metric=\"euclidean\")\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=False,percentile=95)\n",
    "jaccard_metric = MeanIoU(include_background=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your training/val/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33measonqin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xmli/xinyuexu/HW2/wandb/run-20240418_194652-9qp03sn1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/easonqin/homework2/runs/9qp03sn1' target=\"_blank\">Trial1</a></strong> to <a href='https://wandb.ai/easonqin/homework2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/easonqin/homework2' target=\"_blank\">https://wandb.ai/easonqin/homework2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/easonqin/homework2/runs/9qp03sn1' target=\"_blank\">https://wandb.ai/easonqin/homework2/runs/9qp03sn1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1 average loss: 0.6310\n",
      "----------\n",
      "epoch 2 average loss: 0.5678\n",
      "----------\n",
      "epoch 3 average loss: 0.5718\n",
      "----------\n",
      "epoch 4 average loss: 0.5675\n",
      "----------\n",
      "epoch 5 average loss: 0.6023\n",
      "----------\n",
      "epoch 6 average loss: 0.5545\n",
      "----------\n",
      "epoch 7 average loss: 0.5521\n",
      "----------\n",
      "epoch 8 average loss: 0.5581\n",
      "----------\n",
      "epoch 9 average loss: 0.5490\n",
      "----------\n",
      "epoch 10 average loss: 0.5607\n",
      "----------\n",
      "epoch 11 average loss: 0.5478\n",
      "----------\n",
      "epoch 12 average loss: 0.5415\n",
      "----------\n",
      "epoch 13 average loss: 0.5343\n",
      "----------\n",
      "epoch 14 average loss: 0.5380\n",
      "----------\n",
      "epoch 15 average loss: 0.5145\n",
      "----------\n",
      "epoch 16 average loss: 0.5282\n",
      "----------\n",
      "epoch 17 average loss: 0.5182\n",
      "----------\n",
      "epoch 18 average loss: 0.5228\n",
      "----------\n",
      "epoch 19 average loss: 0.5281\n",
      "----------\n",
      "epoch 20 average loss: 0.5211\n",
      "----------\n",
      "epoch 21 average loss: 0.5140\n",
      "----------\n",
      "epoch 22 average loss: 0.5311\n",
      "----------\n",
      "epoch 23 average loss: 0.5092\n",
      "----------\n",
      "epoch 24 average loss: 0.5352\n",
      "----------\n",
      "epoch 25 average loss: 0.5158\n",
      "----------\n",
      "epoch 26 average loss: 0.5156\n",
      "----------\n",
      "epoch 27 average loss: 0.5178\n",
      "----------\n",
      "epoch 28 average loss: 0.4963\n",
      "----------\n",
      "epoch 29 average loss: 0.5074\n",
      "----------\n",
      "epoch 30 average loss: 0.5063\n",
      "----------\n",
      "epoch 31 average loss: 0.5435\n",
      "----------\n",
      "epoch 32 average loss: 0.5067\n",
      "----------\n",
      "epoch 33 average loss: 0.4872\n",
      "----------\n",
      "epoch 34 average loss: 0.4798\n",
      "----------\n",
      "epoch 35 average loss: 0.4759\n",
      "----------\n",
      "epoch 36 average loss: 0.5127\n",
      "----------\n",
      "epoch 37 average loss: 0.5051\n",
      "----------\n",
      "epoch 38 average loss: 0.4756\n",
      "----------\n",
      "epoch 39 average loss: 0.4961\n",
      "----------\n",
      "epoch 40 average loss: 0.4654\n",
      "----------\n",
      "epoch 41 average loss: 0.4849\n",
      "----------\n",
      "epoch 42 average loss: 0.4907\n",
      "----------\n",
      "epoch 43 average loss: 0.4671\n",
      "----------\n",
      "epoch 44 average loss: 0.4793\n",
      "----------\n",
      "epoch 45 average loss: 0.4668\n",
      "----------\n",
      "epoch 46 average loss: 0.4882\n",
      "----------\n",
      "epoch 47 average loss: 0.4634\n",
      "----------\n",
      "epoch 48 average loss: 0.4594\n",
      "----------\n",
      "epoch 49 average loss: 0.4712\n",
      "----------\n",
      "epoch 50 average loss: 0.5005\n",
      "saved new best metric model\n",
      "current epoch: 50 current mean dice: 0.0746\n",
      "best mean dice: 0.0746 at epoch: 50\n",
      "----------\n",
      "epoch 51 average loss: 0.4716\n",
      "----------\n",
      "epoch 52 average loss: 0.4560\n",
      "----------\n",
      "epoch 53 average loss: 0.4495\n",
      "----------\n",
      "epoch 54 average loss: 0.4699\n",
      "----------\n",
      "epoch 55 average loss: 0.4686\n",
      "----------\n",
      "epoch 56 average loss: 0.4606\n",
      "----------\n",
      "epoch 57 average loss: 0.4736\n",
      "----------\n",
      "epoch 58 average loss: 0.4481\n",
      "----------\n",
      "epoch 59 average loss: 0.4824\n",
      "----------\n",
      "epoch 60 average loss: 0.4317\n",
      "----------\n",
      "epoch 61 average loss: 0.4380\n",
      "----------\n",
      "epoch 62 average loss: 0.4538\n",
      "----------\n",
      "epoch 63 average loss: 0.4029\n",
      "----------\n",
      "epoch 64 average loss: 0.4665\n",
      "----------\n",
      "epoch 65 average loss: 0.4766\n",
      "----------\n",
      "epoch 66 average loss: 0.4095\n",
      "----------\n",
      "epoch 67 average loss: 0.4457\n",
      "----------\n",
      "epoch 68 average loss: 0.4049\n",
      "----------\n",
      "epoch 69 average loss: 0.3873\n",
      "----------\n",
      "epoch 70 average loss: 0.4085\n",
      "----------\n",
      "epoch 71 average loss: 0.3986\n",
      "----------\n",
      "epoch 72 average loss: 0.4283\n",
      "----------\n",
      "epoch 73 average loss: 0.4386\n",
      "----------\n",
      "epoch 74 average loss: 0.3989\n",
      "----------\n",
      "epoch 75 average loss: 0.4321\n",
      "----------\n",
      "epoch 76 average loss: 0.3772\n",
      "----------\n",
      "epoch 77 average loss: 0.3945\n",
      "----------\n",
      "epoch 78 average loss: 0.3685\n",
      "----------\n",
      "epoch 79 average loss: 0.3716\n",
      "----------\n",
      "epoch 80 average loss: 0.4161\n",
      "----------\n",
      "epoch 81 average loss: 0.4156\n",
      "----------\n",
      "epoch 82 average loss: 0.3851\n",
      "----------\n",
      "epoch 83 average loss: 0.4099\n",
      "----------\n",
      "epoch 84 average loss: 0.3732\n",
      "----------\n",
      "epoch 85 average loss: 0.4087\n",
      "----------\n",
      "epoch 86 average loss: 0.3800\n",
      "----------\n",
      "epoch 87 average loss: 0.4027\n",
      "----------\n",
      "epoch 88 average loss: 0.3654\n",
      "----------\n",
      "epoch 89 average loss: 0.3737\n",
      "----------\n",
      "epoch 90 average loss: 0.3178\n",
      "----------\n",
      "epoch 91 average loss: 0.3606\n",
      "----------\n",
      "epoch 92 average loss: 0.3259\n",
      "----------\n",
      "epoch 93 average loss: 0.3441\n",
      "----------\n",
      "epoch 94 average loss: 0.3289\n",
      "----------\n",
      "epoch 95 average loss: 0.3590\n",
      "----------\n",
      "epoch 96 average loss: 0.3553\n",
      "----------\n",
      "epoch 97 average loss: 0.3404\n",
      "----------\n",
      "epoch 98 average loss: 0.3397\n",
      "----------\n",
      "epoch 99 average loss: 0.3656\n",
      "----------\n",
      "epoch 100 average loss: 0.3341\n",
      "saved new best metric model\n",
      "current epoch: 100 current mean dice: 0.5569\n",
      "best mean dice: 0.5569 at epoch: 100\n",
      "----------\n",
      "epoch 101 average loss: 0.3356\n",
      "----------\n",
      "epoch 102 average loss: 0.3477\n",
      "----------\n",
      "epoch 103 average loss: 0.3882\n",
      "----------\n",
      "epoch 104 average loss: 0.4417\n",
      "----------\n",
      "epoch 105 average loss: 0.3146\n",
      "----------\n",
      "epoch 106 average loss: 0.3681\n",
      "----------\n",
      "epoch 107 average loss: 0.3480\n",
      "----------\n",
      "epoch 108 average loss: 0.3975\n",
      "----------\n",
      "epoch 109 average loss: 0.3491\n",
      "----------\n",
      "epoch 110 average loss: 0.3683\n",
      "----------\n",
      "epoch 111 average loss: 0.3373\n",
      "----------\n",
      "epoch 112 average loss: 0.3261\n",
      "----------\n",
      "epoch 113 average loss: 0.3484\n",
      "----------\n",
      "epoch 114 average loss: 0.3458\n",
      "----------\n",
      "epoch 115 average loss: 0.3203\n",
      "----------\n",
      "epoch 116 average loss: 0.3437\n",
      "----------\n",
      "epoch 117 average loss: 0.3829\n",
      "----------\n",
      "epoch 118 average loss: 0.3284\n",
      "----------\n",
      "epoch 119 average loss: 0.3348\n",
      "----------\n",
      "epoch 120 average loss: 0.3066\n",
      "----------\n",
      "epoch 121 average loss: 0.2621\n",
      "----------\n",
      "epoch 122 average loss: 0.3125\n",
      "----------\n",
      "epoch 123 average loss: 0.3383\n",
      "----------\n",
      "epoch 124 average loss: 0.2995\n",
      "----------\n",
      "epoch 125 average loss: 0.3450\n",
      "----------\n",
      "epoch 126 average loss: 0.3229\n",
      "----------\n",
      "epoch 127 average loss: 0.2624\n",
      "----------\n",
      "epoch 128 average loss: 0.3176\n",
      "----------\n",
      "epoch 129 average loss: 0.2937\n",
      "----------\n",
      "epoch 130 average loss: 0.3392\n",
      "----------\n",
      "epoch 131 average loss: 0.2923\n",
      "----------\n",
      "epoch 132 average loss: 0.3181\n",
      "----------\n",
      "epoch 133 average loss: 0.3142\n",
      "----------\n",
      "epoch 134 average loss: 0.2587\n",
      "----------\n",
      "epoch 135 average loss: 0.2974\n",
      "----------\n",
      "epoch 136 average loss: 0.3040\n",
      "----------\n",
      "epoch 137 average loss: 0.3015\n",
      "----------\n",
      "epoch 138 average loss: 0.2853\n",
      "----------\n",
      "epoch 139 average loss: 0.3372\n",
      "----------\n",
      "epoch 140 average loss: 0.2771\n",
      "----------\n",
      "epoch 141 average loss: 0.3324\n",
      "----------\n",
      "epoch 142 average loss: 0.2729\n",
      "----------\n",
      "epoch 143 average loss: 0.2783\n",
      "----------\n",
      "epoch 144 average loss: 0.2966\n",
      "----------\n",
      "epoch 145 average loss: 0.2509\n",
      "----------\n",
      "epoch 146 average loss: 0.3473\n",
      "----------\n",
      "epoch 147 average loss: 0.2959\n",
      "----------\n",
      "epoch 148 average loss: 0.2817\n",
      "----------\n",
      "epoch 149 average loss: 0.3513\n",
      "----------\n",
      "epoch 150 average loss: 0.2966\n",
      "current epoch: 150 current mean dice: 0.4180\n",
      "best mean dice: 0.5569 at epoch: 100\n",
      "----------\n",
      "epoch 151 average loss: 0.2815\n",
      "----------\n",
      "epoch 152 average loss: 0.2994\n",
      "----------\n",
      "epoch 153 average loss: 0.3018\n",
      "----------\n",
      "epoch 154 average loss: 0.2615\n",
      "----------\n",
      "epoch 155 average loss: 0.2419\n",
      "----------\n",
      "epoch 156 average loss: 0.2843\n",
      "----------\n",
      "epoch 157 average loss: 0.3280\n",
      "----------\n",
      "epoch 158 average loss: 0.3012\n",
      "----------\n",
      "epoch 159 average loss: 0.2359\n",
      "----------\n",
      "epoch 160 average loss: 0.3000\n",
      "----------\n",
      "epoch 161 average loss: 0.2585\n",
      "----------\n",
      "epoch 162 average loss: 0.3138\n",
      "----------\n",
      "epoch 163 average loss: 0.2263\n",
      "----------\n",
      "epoch 164 average loss: 0.2726\n",
      "----------\n",
      "epoch 165 average loss: 0.3255\n",
      "----------\n",
      "epoch 166 average loss: 0.2355\n",
      "----------\n",
      "epoch 167 average loss: 0.2408\n",
      "----------\n",
      "epoch 168 average loss: 0.2755\n",
      "----------\n",
      "epoch 169 average loss: 0.2717\n",
      "----------\n",
      "epoch 170 average loss: 0.2379\n",
      "----------\n",
      "epoch 171 average loss: 0.2873\n",
      "----------\n",
      "epoch 172 average loss: 0.3066\n",
      "----------\n",
      "epoch 173 average loss: 0.2078\n",
      "----------\n",
      "epoch 174 average loss: 0.3084\n",
      "----------\n",
      "epoch 175 average loss: 0.2090\n",
      "----------\n",
      "epoch 176 average loss: 0.2407\n",
      "----------\n",
      "epoch 177 average loss: 0.2990\n",
      "----------\n",
      "epoch 178 average loss: 0.2478\n",
      "----------\n",
      "epoch 179 average loss: 0.2311\n",
      "----------\n",
      "epoch 180 average loss: 0.2567\n",
      "----------\n",
      "epoch 181 average loss: 0.2527\n",
      "----------\n",
      "epoch 182 average loss: 0.2687\n",
      "----------\n",
      "epoch 183 average loss: 0.2641\n",
      "----------\n",
      "epoch 184 average loss: 0.2754\n",
      "----------\n",
      "epoch 185 average loss: 0.2270\n",
      "----------\n",
      "epoch 186 average loss: 0.2200\n",
      "----------\n",
      "epoch 187 average loss: 0.3017\n",
      "----------\n",
      "epoch 188 average loss: 0.1853\n",
      "----------\n",
      "epoch 189 average loss: 0.2264\n",
      "----------\n",
      "epoch 190 average loss: 0.2150\n",
      "----------\n",
      "epoch 191 average loss: 0.2149\n",
      "----------\n",
      "epoch 192 average loss: 0.2376\n",
      "----------\n",
      "epoch 193 average loss: 0.1893\n",
      "----------\n",
      "epoch 194 average loss: 0.3337\n",
      "----------\n",
      "epoch 195 average loss: 0.2270\n",
      "----------\n",
      "epoch 196 average loss: 0.2673\n",
      "----------\n",
      "epoch 197 average loss: 0.2472\n",
      "----------\n",
      "epoch 198 average loss: 0.1835\n",
      "----------\n",
      "epoch 199 average loss: 0.2498\n",
      "----------\n",
      "epoch 200 average loss: 0.2286\n",
      "saved new best metric model\n",
      "current epoch: 200 current mean dice: 0.8011\n",
      "best mean dice: 0.8011 at epoch: 200\n",
      "----------\n",
      "epoch 201 average loss: 0.2003\n",
      "----------\n",
      "epoch 202 average loss: 0.2633\n",
      "----------\n",
      "epoch 203 average loss: 0.1870\n",
      "----------\n",
      "epoch 204 average loss: 0.3039\n",
      "----------\n",
      "epoch 205 average loss: 0.2425\n",
      "----------\n",
      "epoch 206 average loss: 0.2148\n",
      "----------\n",
      "epoch 207 average loss: 0.2552\n",
      "----------\n",
      "epoch 208 average loss: 0.2050\n",
      "----------\n",
      "epoch 209 average loss: 0.2667\n",
      "----------\n",
      "epoch 210 average loss: 0.1758\n",
      "----------\n",
      "epoch 211 average loss: 0.1807\n",
      "----------\n",
      "epoch 212 average loss: 0.2925\n",
      "----------\n",
      "epoch 213 average loss: 0.2591\n",
      "----------\n",
      "epoch 214 average loss: 0.2533\n",
      "----------\n",
      "epoch 215 average loss: 0.2747\n",
      "----------\n",
      "epoch 216 average loss: 0.1924\n",
      "----------\n",
      "epoch 217 average loss: 0.2753\n",
      "----------\n",
      "epoch 218 average loss: 0.1977\n",
      "----------\n",
      "epoch 219 average loss: 0.2661\n",
      "----------\n",
      "epoch 220 average loss: 0.2590\n",
      "----------\n",
      "epoch 221 average loss: 0.1665\n",
      "----------\n",
      "epoch 222 average loss: 0.1857\n",
      "----------\n",
      "epoch 223 average loss: 0.2119\n",
      "----------\n",
      "epoch 224 average loss: 0.2687\n",
      "----------\n",
      "epoch 225 average loss: 0.2364\n",
      "----------\n",
      "epoch 226 average loss: 0.1751\n",
      "----------\n",
      "epoch 227 average loss: 0.3202\n",
      "----------\n",
      "epoch 228 average loss: 0.2403\n",
      "----------\n",
      "epoch 229 average loss: 0.2045\n",
      "----------\n",
      "epoch 230 average loss: 0.2444\n",
      "----------\n",
      "epoch 231 average loss: 0.2241\n",
      "----------\n",
      "epoch 232 average loss: 0.1368\n",
      "----------\n",
      "epoch 233 average loss: 0.2577\n",
      "----------\n",
      "epoch 234 average loss: 0.2393\n",
      "----------\n",
      "epoch 235 average loss: 0.2306\n",
      "----------\n",
      "epoch 236 average loss: 0.2023\n",
      "----------\n",
      "epoch 237 average loss: 0.2194\n",
      "----------\n",
      "epoch 238 average loss: 0.2188\n",
      "----------\n",
      "epoch 239 average loss: 0.1271\n",
      "----------\n",
      "epoch 240 average loss: 0.1817\n",
      "----------\n",
      "epoch 241 average loss: 0.1854\n",
      "----------\n",
      "epoch 242 average loss: 0.2194\n",
      "----------\n",
      "epoch 243 average loss: 0.2252\n",
      "----------\n",
      "epoch 244 average loss: 0.1639\n",
      "----------\n",
      "epoch 245 average loss: 0.2053\n",
      "----------\n",
      "epoch 246 average loss: 0.2355\n",
      "----------\n",
      "epoch 247 average loss: 0.2108\n",
      "----------\n",
      "epoch 248 average loss: 0.2437\n",
      "----------\n",
      "epoch 249 average loss: 0.2079\n",
      "----------\n",
      "epoch 250 average loss: 0.2227\n",
      "current epoch: 250 current mean dice: 0.7711\n",
      "best mean dice: 0.8011 at epoch: 200\n",
      "----------\n",
      "epoch 251 average loss: 0.2118\n",
      "----------\n",
      "epoch 252 average loss: 0.2481\n",
      "----------\n",
      "epoch 253 average loss: 0.3190\n",
      "----------\n",
      "epoch 254 average loss: 0.2364\n",
      "----------\n",
      "epoch 255 average loss: 0.2069\n",
      "----------\n",
      "epoch 256 average loss: 0.2492\n",
      "----------\n",
      "epoch 257 average loss: 0.2592\n",
      "----------\n",
      "epoch 258 average loss: 0.1427\n",
      "----------\n",
      "epoch 259 average loss: 0.2034\n",
      "----------\n",
      "epoch 260 average loss: 0.2117\n",
      "----------\n",
      "epoch 261 average loss: 0.2417\n",
      "----------\n",
      "epoch 262 average loss: 0.1525\n",
      "----------\n",
      "epoch 263 average loss: 0.1390\n",
      "----------\n",
      "epoch 264 average loss: 0.1690\n",
      "----------\n",
      "epoch 265 average loss: 0.2374\n",
      "----------\n",
      "epoch 266 average loss: 0.2190\n",
      "----------\n",
      "epoch 267 average loss: 0.2089\n",
      "----------\n",
      "epoch 268 average loss: 0.1791\n",
      "----------\n",
      "epoch 269 average loss: 0.2204\n",
      "----------\n",
      "epoch 270 average loss: 0.2304\n",
      "----------\n",
      "epoch 271 average loss: 0.1984\n",
      "----------\n",
      "epoch 272 average loss: 0.2041\n",
      "----------\n",
      "epoch 273 average loss: 0.1872\n",
      "----------\n",
      "epoch 274 average loss: 0.2307\n",
      "----------\n",
      "epoch 275 average loss: 0.3256\n",
      "----------\n",
      "epoch 276 average loss: 0.1589\n",
      "----------\n",
      "epoch 277 average loss: 0.2067\n",
      "----------\n",
      "epoch 278 average loss: 0.2699\n",
      "----------\n",
      "epoch 279 average loss: 0.1885\n",
      "----------\n",
      "epoch 280 average loss: 0.2516\n",
      "----------\n",
      "epoch 281 average loss: 0.1338\n",
      "----------\n",
      "epoch 282 average loss: 0.1459\n",
      "----------\n",
      "epoch 283 average loss: 0.2600\n",
      "----------\n",
      "epoch 284 average loss: 0.2421\n",
      "----------\n",
      "epoch 285 average loss: 0.2059\n",
      "----------\n",
      "epoch 286 average loss: 0.2234\n",
      "----------\n",
      "epoch 287 average loss: 0.1927\n",
      "----------\n",
      "epoch 288 average loss: 0.2862\n",
      "----------\n",
      "epoch 289 average loss: 0.2312\n",
      "----------\n",
      "epoch 290 average loss: 0.2222\n",
      "----------\n",
      "epoch 291 average loss: 0.2106\n",
      "----------\n",
      "epoch 292 average loss: 0.1678\n",
      "----------\n",
      "epoch 293 average loss: 0.2340\n",
      "----------\n",
      "epoch 294 average loss: 0.2144\n",
      "----------\n",
      "epoch 295 average loss: 0.1984\n",
      "----------\n",
      "epoch 296 average loss: 0.2239\n",
      "----------\n",
      "epoch 297 average loss: 0.1932\n",
      "----------\n",
      "epoch 298 average loss: 0.2067\n",
      "----------\n",
      "epoch 299 average loss: 0.1703\n",
      "----------\n",
      "epoch 300 average loss: 0.2532\n",
      "saved new best metric model\n",
      "current epoch: 300 current mean dice: 0.8786\n",
      "best mean dice: 0.8786 at epoch: 300\n",
      "----------\n",
      "epoch 301 average loss: 0.1658\n",
      "----------\n",
      "epoch 302 average loss: 0.1780\n",
      "----------\n",
      "epoch 303 average loss: 0.1420\n",
      "----------\n",
      "epoch 304 average loss: 0.2106\n",
      "----------\n",
      "epoch 305 average loss: 0.2042\n",
      "----------\n",
      "epoch 306 average loss: 0.1562\n",
      "----------\n",
      "epoch 307 average loss: 0.2379\n",
      "----------\n",
      "epoch 308 average loss: 0.1937\n",
      "----------\n",
      "epoch 309 average loss: 0.1843\n",
      "----------\n",
      "epoch 310 average loss: 0.1787\n",
      "----------\n",
      "epoch 311 average loss: 0.2554\n",
      "----------\n",
      "epoch 312 average loss: 0.2828\n",
      "----------\n",
      "epoch 313 average loss: 0.1880\n",
      "----------\n",
      "epoch 314 average loss: 0.1942\n",
      "----------\n",
      "epoch 315 average loss: 0.2562\n",
      "----------\n",
      "epoch 316 average loss: 0.2219\n",
      "----------\n",
      "epoch 317 average loss: 0.2578\n",
      "----------\n",
      "epoch 318 average loss: 0.1736\n",
      "----------\n",
      "epoch 319 average loss: 0.1793\n",
      "----------\n",
      "epoch 320 average loss: 0.2108\n",
      "----------\n",
      "epoch 321 average loss: 0.2633\n",
      "----------\n",
      "epoch 322 average loss: 0.2892\n",
      "----------\n",
      "epoch 323 average loss: 0.2223\n",
      "----------\n",
      "epoch 324 average loss: 0.2238\n",
      "----------\n",
      "epoch 325 average loss: 0.1997\n",
      "----------\n",
      "epoch 326 average loss: 0.1546\n",
      "----------\n",
      "epoch 327 average loss: 0.1914\n",
      "----------\n",
      "epoch 328 average loss: 0.2300\n",
      "----------\n",
      "epoch 329 average loss: 0.1860\n",
      "----------\n",
      "epoch 330 average loss: 0.1603\n",
      "----------\n",
      "epoch 331 average loss: 0.1302\n",
      "----------\n",
      "epoch 332 average loss: 0.1435\n",
      "----------\n",
      "epoch 333 average loss: 0.1498\n",
      "----------\n",
      "epoch 334 average loss: 0.2328\n",
      "----------\n",
      "epoch 335 average loss: 0.2132\n",
      "----------\n",
      "epoch 336 average loss: 0.2001\n",
      "----------\n",
      "epoch 337 average loss: 0.1404\n",
      "----------\n",
      "epoch 338 average loss: 0.2666\n",
      "----------\n",
      "epoch 339 average loss: 0.2524\n",
      "----------\n",
      "epoch 340 average loss: 0.2387\n",
      "----------\n",
      "epoch 341 average loss: 0.2242\n",
      "----------\n",
      "epoch 342 average loss: 0.1859\n",
      "----------\n",
      "epoch 343 average loss: 0.2166\n",
      "----------\n",
      "epoch 344 average loss: 0.1691\n",
      "----------\n",
      "epoch 345 average loss: 0.1096\n",
      "----------\n",
      "epoch 346 average loss: 0.1248\n",
      "----------\n",
      "epoch 347 average loss: 0.1724\n",
      "----------\n",
      "epoch 348 average loss: 0.1498\n",
      "----------\n",
      "epoch 349 average loss: 0.2080\n",
      "----------\n",
      "epoch 350 average loss: 0.2320\n",
      "current epoch: 350 current mean dice: 0.8124\n",
      "best mean dice: 0.8786 at epoch: 300\n",
      "----------\n",
      "epoch 351 average loss: 0.2675\n",
      "----------\n",
      "epoch 352 average loss: 0.2401\n",
      "----------\n",
      "epoch 353 average loss: 0.2181\n",
      "----------\n",
      "epoch 354 average loss: 0.1346\n",
      "----------\n",
      "epoch 355 average loss: 0.2345\n",
      "----------\n",
      "epoch 356 average loss: 0.2600\n",
      "----------\n",
      "epoch 357 average loss: 0.1664\n",
      "----------\n",
      "epoch 358 average loss: 0.1987\n",
      "----------\n",
      "epoch 359 average loss: 0.3246\n",
      "----------\n",
      "epoch 360 average loss: 0.1956\n",
      "----------\n",
      "epoch 361 average loss: 0.1741\n",
      "----------\n",
      "epoch 362 average loss: 0.1933\n",
      "----------\n",
      "epoch 363 average loss: 0.2167\n",
      "----------\n",
      "epoch 364 average loss: 0.1995\n",
      "----------\n",
      "epoch 365 average loss: 0.1803\n",
      "----------\n",
      "epoch 366 average loss: 0.1404\n",
      "----------\n",
      "epoch 367 average loss: 0.1457\n",
      "----------\n",
      "epoch 368 average loss: 0.2212\n",
      "----------\n",
      "epoch 369 average loss: 0.1522\n",
      "----------\n",
      "epoch 370 average loss: 0.2708\n",
      "----------\n",
      "epoch 371 average loss: 0.1750\n",
      "----------\n",
      "epoch 372 average loss: 0.1599\n",
      "----------\n",
      "epoch 373 average loss: 0.1406\n",
      "----------\n",
      "epoch 374 average loss: 0.2492\n",
      "----------\n",
      "epoch 375 average loss: 0.1413\n",
      "----------\n",
      "epoch 376 average loss: 0.2774\n",
      "----------\n",
      "epoch 377 average loss: 0.2155\n",
      "----------\n",
      "epoch 378 average loss: 0.2807\n",
      "----------\n",
      "epoch 379 average loss: 0.2172\n",
      "----------\n",
      "epoch 380 average loss: 0.2430\n",
      "----------\n",
      "epoch 381 average loss: 0.2134\n",
      "----------\n",
      "epoch 382 average loss: 0.2201\n",
      "----------\n",
      "epoch 383 average loss: 0.2087\n",
      "----------\n",
      "epoch 384 average loss: 0.2150\n",
      "----------\n",
      "epoch 385 average loss: 0.1310\n",
      "----------\n",
      "epoch 386 average loss: 0.1326\n",
      "----------\n",
      "epoch 387 average loss: 0.1812\n",
      "----------\n",
      "epoch 388 average loss: 0.1949\n",
      "----------\n",
      "epoch 389 average loss: 0.1167\n",
      "----------\n",
      "epoch 390 average loss: 0.1552\n",
      "----------\n",
      "epoch 391 average loss: 0.1174\n",
      "----------\n",
      "epoch 392 average loss: 0.1022\n",
      "----------\n",
      "epoch 393 average loss: 0.1436\n",
      "----------\n",
      "epoch 394 average loss: 0.1583\n",
      "----------\n",
      "epoch 395 average loss: 0.1191\n",
      "----------\n",
      "epoch 396 average loss: 0.1108\n",
      "----------\n",
      "epoch 397 average loss: 0.1977\n",
      "----------\n",
      "epoch 398 average loss: 0.1640\n",
      "----------\n",
      "epoch 399 average loss: 0.2147\n",
      "----------\n",
      "epoch 400 average loss: 0.1309\n",
      "saved new best metric model\n",
      "current epoch: 400 current mean dice: 0.8862\n",
      "best mean dice: 0.8862 at epoch: 400\n",
      "----------\n",
      "epoch 401 average loss: 0.2258\n",
      "----------\n",
      "epoch 402 average loss: 0.1957\n",
      "----------\n",
      "epoch 403 average loss: 0.1444\n",
      "----------\n",
      "epoch 404 average loss: 0.2509\n",
      "----------\n",
      "epoch 405 average loss: 0.1881\n",
      "----------\n",
      "epoch 406 average loss: 0.2053\n",
      "----------\n",
      "epoch 407 average loss: 0.1424\n",
      "----------\n",
      "epoch 408 average loss: 0.1404\n",
      "----------\n",
      "epoch 409 average loss: 0.1586\n",
      "----------\n",
      "epoch 410 average loss: 0.1011\n",
      "----------\n",
      "epoch 411 average loss: 0.1606\n",
      "----------\n",
      "epoch 412 average loss: 0.2899\n",
      "----------\n",
      "epoch 413 average loss: 0.1994\n",
      "----------\n",
      "epoch 414 average loss: 0.2315\n",
      "----------\n",
      "epoch 415 average loss: 0.1424\n",
      "----------\n",
      "epoch 416 average loss: 0.2241\n",
      "----------\n",
      "epoch 417 average loss: 0.2482\n",
      "----------\n",
      "epoch 418 average loss: 0.2143\n",
      "----------\n",
      "epoch 419 average loss: 0.1620\n",
      "----------\n",
      "epoch 420 average loss: 0.1469\n",
      "----------\n",
      "epoch 421 average loss: 0.2314\n",
      "----------\n",
      "epoch 422 average loss: 0.2155\n",
      "----------\n",
      "epoch 423 average loss: 0.1200\n",
      "----------\n",
      "epoch 424 average loss: 0.1862\n",
      "----------\n",
      "epoch 425 average loss: 0.1263\n",
      "----------\n",
      "epoch 426 average loss: 0.1986\n",
      "----------\n",
      "epoch 427 average loss: 0.2029\n",
      "----------\n",
      "epoch 428 average loss: 0.2082\n",
      "----------\n",
      "epoch 429 average loss: 0.2393\n",
      "----------\n",
      "epoch 430 average loss: 0.1543\n",
      "----------\n",
      "epoch 431 average loss: 0.1896\n",
      "----------\n",
      "epoch 432 average loss: 0.1907\n",
      "----------\n",
      "epoch 433 average loss: 0.2371\n",
      "----------\n",
      "epoch 434 average loss: 0.2260\n",
      "----------\n",
      "epoch 435 average loss: 0.2090\n",
      "----------\n",
      "epoch 436 average loss: 0.1227\n",
      "----------\n",
      "epoch 437 average loss: 0.2112\n",
      "----------\n",
      "epoch 438 average loss: 0.2199\n",
      "----------\n",
      "epoch 439 average loss: 0.1900\n",
      "----------\n",
      "epoch 440 average loss: 0.0991\n",
      "----------\n",
      "epoch 441 average loss: 0.1396\n",
      "----------\n",
      "epoch 442 average loss: 0.1805\n",
      "----------\n",
      "epoch 443 average loss: 0.1554\n",
      "----------\n",
      "epoch 444 average loss: 0.1952\n",
      "----------\n",
      "epoch 445 average loss: 0.2178\n",
      "----------\n",
      "epoch 446 average loss: 0.1868\n",
      "----------\n",
      "epoch 447 average loss: 0.1344\n",
      "----------\n",
      "epoch 448 average loss: 0.1877\n",
      "----------\n",
      "epoch 449 average loss: 0.1190\n",
      "----------\n",
      "epoch 450 average loss: 0.2575\n",
      "current epoch: 450 current mean dice: 0.8240\n",
      "best mean dice: 0.8862 at epoch: 400\n",
      "----------\n",
      "epoch 451 average loss: 0.1822\n",
      "----------\n",
      "epoch 452 average loss: 0.1737\n",
      "----------\n",
      "epoch 453 average loss: 0.1819\n",
      "----------\n",
      "epoch 454 average loss: 0.2065\n",
      "----------\n",
      "epoch 455 average loss: 0.1379\n",
      "----------\n",
      "epoch 456 average loss: 0.2072\n",
      "----------\n",
      "epoch 457 average loss: 0.1946\n",
      "----------\n",
      "epoch 458 average loss: 0.1989\n",
      "----------\n",
      "epoch 459 average loss: 0.1948\n",
      "----------\n",
      "epoch 460 average loss: 0.1659\n",
      "----------\n",
      "epoch 461 average loss: 0.1644\n",
      "----------\n",
      "epoch 462 average loss: 0.2442\n",
      "----------\n",
      "epoch 463 average loss: 0.2545\n",
      "----------\n",
      "epoch 464 average loss: 0.1327\n",
      "----------\n",
      "epoch 465 average loss: 0.2277\n",
      "----------\n",
      "epoch 466 average loss: 0.1080\n",
      "----------\n",
      "epoch 467 average loss: 0.2691\n",
      "----------\n",
      "epoch 468 average loss: 0.2073\n",
      "----------\n",
      "epoch 469 average loss: 0.2014\n",
      "----------\n",
      "epoch 470 average loss: 0.2095\n",
      "----------\n",
      "epoch 471 average loss: 0.1400\n",
      "----------\n",
      "epoch 472 average loss: 0.2261\n",
      "----------\n",
      "epoch 473 average loss: 0.1253\n",
      "----------\n",
      "epoch 474 average loss: 0.2620\n",
      "----------\n",
      "epoch 475 average loss: 0.1342\n",
      "----------\n",
      "epoch 476 average loss: 0.1555\n",
      "----------\n",
      "epoch 477 average loss: 0.1343\n",
      "----------\n",
      "epoch 478 average loss: 0.2089\n",
      "----------\n",
      "epoch 479 average loss: 0.2631\n",
      "----------\n",
      "epoch 480 average loss: 0.1011\n",
      "----------\n",
      "epoch 481 average loss: 0.1253\n",
      "----------\n",
      "epoch 482 average loss: 0.1329\n",
      "----------\n",
      "epoch 483 average loss: 0.1969\n",
      "----------\n",
      "epoch 484 average loss: 0.1100\n",
      "----------\n",
      "epoch 485 average loss: 0.1865\n",
      "----------\n",
      "epoch 486 average loss: 0.1353\n",
      "----------\n",
      "epoch 487 average loss: 0.1907\n",
      "----------\n",
      "epoch 488 average loss: 0.0967\n",
      "----------\n",
      "epoch 489 average loss: 0.1703\n",
      "----------\n",
      "epoch 490 average loss: 0.2373\n",
      "----------\n",
      "epoch 491 average loss: 0.1170\n",
      "----------\n",
      "epoch 492 average loss: 0.1963\n",
      "----------\n",
      "epoch 493 average loss: 0.1311\n",
      "----------\n",
      "epoch 494 average loss: 0.1786\n",
      "----------\n",
      "epoch 495 average loss: 0.2098\n",
      "----------\n",
      "epoch 496 average loss: 0.1996\n",
      "----------\n",
      "epoch 497 average loss: 0.2933\n",
      "----------\n",
      "epoch 498 average loss: 0.2331\n",
      "----------\n",
      "epoch 499 average loss: 0.1808\n",
      "----------\n",
      "epoch 500 average loss: 0.1968\n",
      "current epoch: 500 current mean dice: 0.8669\n",
      "best mean dice: 0.8862 at epoch: 400\n",
      "----------\n",
      "epoch 501 average loss: 0.2494\n",
      "----------\n",
      "epoch 502 average loss: 0.1166\n",
      "----------\n",
      "epoch 503 average loss: 0.1992\n",
      "----------\n",
      "epoch 504 average loss: 0.2275\n",
      "----------\n",
      "epoch 505 average loss: 0.2089\n",
      "----------\n",
      "epoch 506 average loss: 0.1524\n",
      "----------\n",
      "epoch 507 average loss: 0.2993\n",
      "----------\n",
      "epoch 508 average loss: 0.1088\n",
      "----------\n",
      "epoch 509 average loss: 0.2258\n",
      "----------\n",
      "epoch 510 average loss: 0.1767\n",
      "----------\n",
      "epoch 511 average loss: 0.1447\n",
      "----------\n",
      "epoch 512 average loss: 0.1429\n",
      "----------\n",
      "epoch 513 average loss: 0.0802\n",
      "----------\n",
      "epoch 514 average loss: 0.1418\n",
      "----------\n",
      "epoch 515 average loss: 0.2069\n",
      "----------\n",
      "epoch 516 average loss: 0.0968\n",
      "----------\n",
      "epoch 517 average loss: 0.1947\n",
      "----------\n",
      "epoch 518 average loss: 0.2091\n",
      "----------\n",
      "epoch 519 average loss: 0.1987\n",
      "----------\n",
      "epoch 520 average loss: 0.1593\n",
      "----------\n",
      "epoch 521 average loss: 0.1421\n",
      "----------\n",
      "epoch 522 average loss: 0.1740\n",
      "----------\n",
      "epoch 523 average loss: 0.1167\n",
      "----------\n",
      "epoch 524 average loss: 0.1152\n",
      "----------\n",
      "epoch 525 average loss: 0.1538\n",
      "----------\n",
      "epoch 526 average loss: 0.2489\n",
      "----------\n",
      "epoch 527 average loss: 0.1898\n",
      "----------\n",
      "epoch 528 average loss: 0.2632\n",
      "----------\n",
      "epoch 529 average loss: 0.1460\n",
      "----------\n",
      "epoch 530 average loss: 0.2294\n",
      "----------\n",
      "epoch 531 average loss: 0.2612\n",
      "----------\n",
      "epoch 532 average loss: 0.1817\n",
      "----------\n",
      "epoch 533 average loss: 0.1869\n",
      "----------\n",
      "epoch 534 average loss: 0.2661\n",
      "----------\n",
      "epoch 535 average loss: 0.1744\n",
      "----------\n",
      "epoch 536 average loss: 0.2075\n",
      "----------\n",
      "epoch 537 average loss: 0.1710\n",
      "----------\n",
      "epoch 538 average loss: 0.1247\n",
      "----------\n",
      "epoch 539 average loss: 0.1202\n",
      "----------\n",
      "epoch 540 average loss: 0.2257\n",
      "----------\n",
      "epoch 541 average loss: 0.1370\n",
      "----------\n",
      "epoch 542 average loss: 0.1132\n",
      "----------\n",
      "epoch 543 average loss: 0.1205\n",
      "----------\n",
      "epoch 544 average loss: 0.2155\n",
      "----------\n",
      "epoch 545 average loss: 0.1519\n",
      "----------\n",
      "epoch 546 average loss: 0.2045\n",
      "----------\n",
      "epoch 547 average loss: 0.1821\n",
      "----------\n",
      "epoch 548 average loss: 0.2291\n",
      "----------\n",
      "epoch 549 average loss: 0.1624\n",
      "----------\n",
      "epoch 550 average loss: 0.1953\n",
      "current epoch: 550 current mean dice: 0.8070\n",
      "best mean dice: 0.8862 at epoch: 400\n",
      "----------\n",
      "epoch 551 average loss: 0.1177\n",
      "----------\n",
      "epoch 552 average loss: 0.1912\n",
      "----------\n",
      "epoch 553 average loss: 0.1582\n",
      "----------\n",
      "epoch 554 average loss: 0.1802\n",
      "----------\n",
      "epoch 555 average loss: 0.1623\n",
      "----------\n",
      "epoch 556 average loss: 0.2206\n",
      "----------\n",
      "epoch 557 average loss: 0.1710\n",
      "----------\n",
      "epoch 558 average loss: 0.2229\n",
      "----------\n",
      "epoch 559 average loss: 0.1403\n",
      "----------\n",
      "epoch 560 average loss: 0.1519\n",
      "----------\n",
      "epoch 561 average loss: 0.1820\n",
      "----------\n",
      "epoch 562 average loss: 0.1662\n",
      "----------\n",
      "epoch 563 average loss: 0.2627\n",
      "----------\n",
      "epoch 564 average loss: 0.1538\n",
      "----------\n",
      "epoch 565 average loss: 0.1815\n",
      "----------\n",
      "epoch 566 average loss: 0.1702\n",
      "----------\n",
      "epoch 567 average loss: 0.2309\n",
      "----------\n",
      "epoch 568 average loss: 0.2099\n",
      "----------\n",
      "epoch 569 average loss: 0.2084\n",
      "----------\n",
      "epoch 570 average loss: 0.1973\n",
      "----------\n",
      "epoch 571 average loss: 0.0775\n",
      "----------\n",
      "epoch 572 average loss: 0.1386\n",
      "----------\n",
      "epoch 573 average loss: 0.0900\n",
      "----------\n",
      "epoch 574 average loss: 0.1364\n",
      "----------\n",
      "epoch 575 average loss: 0.1943\n",
      "----------\n",
      "epoch 576 average loss: 0.1828\n",
      "----------\n",
      "epoch 577 average loss: 0.1808\n",
      "----------\n",
      "epoch 578 average loss: 0.1402\n",
      "----------\n",
      "epoch 579 average loss: 0.1949\n",
      "----------\n",
      "epoch 580 average loss: 0.1512\n",
      "----------\n",
      "epoch 581 average loss: 0.1525\n",
      "----------\n",
      "epoch 582 average loss: 0.1338\n",
      "----------\n",
      "epoch 583 average loss: 0.1741\n",
      "----------\n",
      "epoch 584 average loss: 0.1700\n",
      "----------\n",
      "epoch 585 average loss: 0.1199\n",
      "----------\n",
      "epoch 586 average loss: 0.1261\n",
      "----------\n",
      "epoch 587 average loss: 0.2081\n",
      "----------\n",
      "epoch 588 average loss: 0.2252\n",
      "----------\n",
      "epoch 589 average loss: 0.1908\n",
      "----------\n",
      "epoch 590 average loss: 0.1922\n",
      "----------\n",
      "epoch 591 average loss: 0.0948\n",
      "----------\n",
      "epoch 592 average loss: 0.2337\n",
      "----------\n",
      "epoch 593 average loss: 0.1850\n",
      "----------\n",
      "epoch 594 average loss: 0.1521\n",
      "----------\n",
      "epoch 595 average loss: 0.1566\n",
      "----------\n",
      "epoch 596 average loss: 0.2285\n",
      "----------\n",
      "epoch 597 average loss: 0.2495\n",
      "----------\n",
      "epoch 598 average loss: 0.2105\n",
      "----------\n",
      "epoch 599 average loss: 0.1341\n",
      "----------\n",
      "epoch 600 average loss: 0.1520\n",
      "saved new best metric model\n",
      "current epoch: 600 current mean dice: 0.9391\n",
      "best mean dice: 0.9391 at epoch: 600\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"homework2\", name=\"Trial1\")\n",
    "\n",
    "max_epochs = 600\n",
    "val_interval = 50\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "dice_values = []\n",
    "asd_values = []\n",
    "hausdorff_values = []\n",
    "jaccard_values = []\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[0][\"image\"].to(device),\n",
    "            batch_data[0][\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    wandb.log({'train_loss': epoch_loss}, step= epoch)\n",
    "\n",
    "    # ONE POSSIBLE VALIDATION METHOD\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(val_output) for val_output in val_outputs]\n",
    "                val_labels = [post_label(val_label) for val_label in val_labels]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                asd_metric(y_pred=val_outputs, y=val_labels)\n",
    "                hausdorff_metric(y_pred=val_outputs, y=val_labels)\n",
    "                jaccard_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            dice = dice_metric.aggregate().item()\n",
    "            asd = asd_metric.aggregate().item()\n",
    "            hausdorff = hausdorff_metric.aggregate().item()\n",
    "            jaccard = jaccard_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            asd_metric.reset()\n",
    "            hausdorff_metric.reset()\n",
    "            jaccard_metric.reset()\n",
    "\n",
    "            dice_values.append(dice)\n",
    "            asd_values.append(asd)\n",
    "            hausdorff_values.append(hausdorff)\n",
    "            jaccard_values.append(jaccard)\n",
    "            wandb.log({'val_dice': dice}, step= epoch)\n",
    "            wandb.log({'val_asd': asd}, step= epoch)\n",
    "            wandb.log({'val_hausdorff': hausdorff}, step= epoch)\n",
    "            wandb.log({'val_jaccard': jaccard}, step= epoch)\n",
    "            \n",
    "            if dice > best_metric:\n",
    "                best_metric = dice\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"./best_metric_model.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {dice:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Report performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(save_image,save_path):\n",
    "    save_image=save_image.cpu().numpy()\n",
    "    sitk_img = sitk.GetImageFromArray(save_image)\n",
    "    sitk.WriteImage(sitk_img, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:17<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases results:\n",
      "Average Dice: 0.9414\n",
      "Average ASD: 3.8363\n",
      "Average Hausdorff: 35.9547\n",
      "Average Jaccard: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ONE POSSIBLE TEST METHOD\n",
    "\n",
    "from tqdm import tqdm\n",
    "model.load_state_dict(torch.load(\"./best_metric_model.pth\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_data in tqdm(test_loader):\n",
    "        test_inputs, test_labels = (\n",
    "            test_data[\"image\"].to(device),\n",
    "            test_data[\"label\"].to(device),\n",
    "        )\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        test_outputs = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
    "        # print(test_outputs.shape)\n",
    "        test_outputs = [post_pred(test_output) for test_output in test_outputs]\n",
    "        test_labels = [post_label(test_label) for test_label in test_labels]\n",
    "        # print(test_outputs[0].shape)\n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "        asd_metric(y_pred=test_outputs, y=test_labels)\n",
    "        hausdorff_metric(y_pred=test_outputs, y=test_labels)\n",
    "        jaccard_metric(y_pred=test_outputs, y=test_labels)\n",
    "        \n",
    "    # aggregate the final mean dice result\n",
    "    dice = dice_metric.aggregate().item()\n",
    "    asd = asd_metric.aggregate().item()\n",
    "    hausdorff = hausdorff_metric.aggregate().item()\n",
    "    jaccard = jaccard_metric.aggregate().item()\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()\n",
    "    asd_metric.reset()\n",
    "    hausdorff_metric.reset()\n",
    "    jaccard_metric.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Test cases results:\")\n",
    "# print average results\n",
    "print(f\"Average Dice: {dice:.4f}\")\n",
    "print(f\"Average ASD: {asd:.4f}\")\n",
    "print(f\"Average Hausdorff: {hausdorff:.4f}\")\n",
    "print(f\"Average Jaccard: {jaccard:.4f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "We acknowledge the awesome coding samples provided by MONAI. You are also welcome to check out the other great coding samples provided by them to get familiar with dealing with Medical Imaging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
